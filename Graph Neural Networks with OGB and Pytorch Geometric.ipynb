{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85e104f0-dbc0-40c8-8b21-3160433535a1",
   "metadata": {},
   "source": [
    "# Graph Neural Networks with OGB and Pytorch Geometric\n",
    "\n",
    "In this notebook we are going to implement a Graph Neural Network using pytorch geometric and the Open Graph Benchmark. The goal of this notebook is giving general directions for anyone wishing to start in this kind of development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e3ee95f-a18a-4bed-aa8e-1e3657d4f64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tiago\\anaconda3\\envs\\masters_env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch_geometric.nn import MessagePassing, SAGEConv\n",
    "from ogb.nodeproppred import Evaluator, PygNodePropPredDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb037f1a-5949-4ac3-9522-5d35cf2b7c28",
   "metadata": {},
   "source": [
    "## Open Graph Benchmark (OGB)\n",
    "\n",
    "The OGB is a collection of realistic, large-scale, and diverse benchmark datasets for machine learning on graphs. It gives us curated datasets and formalize the splitting and evaluation process for prediction tasks on those datasets.\n",
    "\n",
    "One can imagine it as the ImageNet dataset for computer vision. Their goal is to create a standard way of evaluating advances on the Graph Learning area.\n",
    "\n",
    "We will use the 'ogbn-arxiv' dataset, which is a node prediction dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c7c1658-1566-459e-8fe5-71da8b206caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PygNodePropPredDataset()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_dataset = 'ogbn-arxiv'\n",
    "\n",
    "# This will download the ogbn-arxiv to the 'networks' folder\n",
    "dataset = PygNodePropPredDataset(name=target_dataset, root='networks')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6de9d0fd-7541-4c35-99dc-3b8457ff1769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data we are going to use can be extracted from the dataset as follows:\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d259586-7339-4fb6-ac99-2f6b4154728a",
   "metadata": {},
   "source": [
    "For graph prediction tasks, each value from the dataset would be a different graph. Here we are dealing with only one graph saved on the 'data' variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08384f4a-9e04-4e98-b03a-0c4efb50cca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(num_nodes=169343, edge_index=[2, 1166243], x=[169343, 128], node_year=[169343, 1], y=[169343, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbe60bd-5733-4095-9a46-6fba9a652471",
   "metadata": {},
   "source": [
    "This is a Data class from Pytorch. Here we can see some information: the number of nodes in the graph, the adjacency list (called edge_index), the feature matrix of the graph (x), the year for each node and the prediction target (y).\n",
    "\n",
    "This is what we are going to use to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86969f82-5de2-45b9-b134-3db98cd9d605",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_idx = dataset.get_idx_split() \n",
    "        \n",
    "train_idx = split_idx['train']\n",
    "valid_idx = split_idx['valid']\n",
    "test_idx = split_idx['test']\n",
    "        \n",
    "train_loader = NeighborLoader(data, input_nodes=train_idx,\n",
    "                              shuffle=True, num_workers=os.cpu_count() - 2,\n",
    "                              batch_size=1024, num_neighbors=[30] * 2)\n",
    "\n",
    "total_loader = NeighborLoader(data, input_nodes=None, num_neighbors=[-1],\n",
    "                                           batch_size=4096, shuffle=False,\n",
    "                                           num_workers=os.cpu_count() - 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9d5d48-23a0-4949-9f18-944762641634",
   "metadata": {},
   "source": [
    "## Pytorch Geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc69732-d958-43b1-a398-3f73cdfa297e",
   "metadata": {},
   "source": [
    "### Creating the GNN\n",
    "\n",
    "We are going to use a SAGE GNN for this notebook. We will allow the number of layers to be parametrized, but will use only two here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc997eba-cfd6-4ce4-ad6a-6e1d70913da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels,\n",
    "                 hidden_channels, out_channels,\n",
    "                 n_layers=2):\n",
    "        \n",
    "        super(SAGE, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.layers_bn = torch.nn.ModuleList()\n",
    "\n",
    "        if n_layers == 1:\n",
    "            self.layers.append(SAGEConv(in_channels, out_channels, normalize=False))\n",
    "        elif n_layers == 2:\n",
    "            self.layers.append(SAGEConv(in_channels, hidden_channels, normalize=False))\n",
    "            self.layers_bn.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "            self.layers.append(SAGEConv(hidden_channels, out_channels, normalize=False))\n",
    "        else:\n",
    "            self.layers.append(SAGEConv(in_channels, hidden_channels, normalize=False))\n",
    "            self.layers_bn.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "            for _ in range(n_layers - 2):\n",
    "                self.layers.append(SAGEConv(hidden_channels, hidden_channels, normalize=False))\n",
    "                self.layers_bn.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "            \n",
    "            self.layers.append(SAGEConv(hidden_channels, out_channels, normalize=False))\n",
    "            \n",
    "        for layer in self.layers:\n",
    "            layer.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        if len(self.layers) > 1:\n",
    "            looper = self.layers[:-1]\n",
    "        else:\n",
    "            looper = self.layers\n",
    "        \n",
    "        for i, layer in enumerate(looper):\n",
    "            x = layer(x, edge_index)\n",
    "            try:\n",
    "                x = self.layers_bn[i](x)\n",
    "            except Exception as e:\n",
    "                abs(1)\n",
    "            finally:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=0.5, training=self.training)\n",
    "        \n",
    "        if len(self.layers) > 1:\n",
    "            x = self.layers[-1](x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=-1), torch.var(x)\n",
    "    \n",
    "    def inference(self, total_loader, device):\n",
    "        xs = []\n",
    "        var_ = []\n",
    "        for batch in total_loader:\n",
    "            out, var = self.forward(batch.x.to(device), batch.edge_index.to(device))\n",
    "            out = out[:batch.batch_size]\n",
    "            xs.append(out.cpu())\n",
    "            var_.append(var.item())\n",
    "        \n",
    "        out_all = torch.cat(xs, dim=0)\n",
    "        \n",
    "        return out_all, var_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a60a67-1695-40f4-91fd-ea1c91e6dd1f",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19c341c0-17ac-48e1-a1dd-a3ac2db454e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = SAGE(data.x.shape[1], 256, dataset.num_classes, n_layers=2)\n",
    "model.to(device)\n",
    "epochs = 100\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.03)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', patience=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57a98c9a-6e43-49bc-926f-faabecf028a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device):\n",
    "    evaluator = Evaluator(name=target_dataset)\n",
    "    model.eval()\n",
    "    out, var = model.inference(total_loader, device)\n",
    "\n",
    "    y_true = data.y.cpu()\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "    train_acc = evaluator.eval({\n",
    "        'y_true': y_true[split_idx['train']],\n",
    "        'y_pred': y_pred[split_idx['train']],\n",
    "    })['acc']\n",
    "    val_acc = evaluator.eval({\n",
    "        'y_true': y_true[split_idx['valid']],\n",
    "        'y_pred': y_pred[split_idx['valid']],\n",
    "    })['acc']\n",
    "    test_acc = evaluator.eval({\n",
    "        'y_true': y_true[split_idx['test']],\n",
    "        'y_pred': y_pred[split_idx['test']],\n",
    "    })['acc']\n",
    "\n",
    "    return train_acc, val_acc, test_acc, torch.mean(torch.Tensor(var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823aeb89-df10-4230-a7a8-751fce764566",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 01: 100%|██████████| 90941/90941 [01:58<00:00, 765.76it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.5777, Val: 0.5667, Test: 0.5087, Var: 8.2168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 02: 100%|██████████| 90941/90941 [02:57<00:00, 512.57it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.5881, Val: 0.5618, Test: 0.5072, Var: 8.4361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 03: 100%|██████████| 90941/90941 [01:28<00:00, 1024.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.6042, Val: 0.5803, Test: 0.5195, Var: 8.1180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 04:  44%|████▍     | 39936/90941 [00:56<00:29, 1710.26it/s]"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs):\n",
    "    model.train()\n",
    "\n",
    "    pbar = tqdm(total=train_idx.size(0))\n",
    "    pbar.set_description(f'Epoch {epoch:02d}')\n",
    "\n",
    "    total_loss = total_correct = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        batch_size = batch.batch_size\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out, _ = model(batch.x.to(device), batch.edge_index.to(device))\n",
    "        out = out[:batch_size]\n",
    "\n",
    "        batch_y = batch.y[:batch_size].to(device)\n",
    "        batch_y = torch.reshape(batch_y, (-1,))\n",
    "\n",
    "        loss = F.nll_loss(out, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss)\n",
    "        total_correct += int(out.argmax(dim=-1).eq(batch_y).sum())\n",
    "        pbar.update(batch.batch_size)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    loss = total_loss / len(train_loader)\n",
    "    approx_acc = total_correct / train_idx.size(0)\n",
    "\n",
    "    train_acc, val_acc, test_acc, var = test(model, device)\n",
    "    \n",
    "    print(f'Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}, Var: {var:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
